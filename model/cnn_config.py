# -*- coding: utf-8 -*-
"""
Created on Sat Sep 11 22:02:14 2021

@author: WKOUBAA
"""

# preprocess
TRAINING_DIR = "training_dir"
#MAX_SEQUENCE_LENGTH = 11
MIN_FREQ_FILTER = 2

MAX_NUM_WORDS = 50000
EMBEDDING_METHOD = 'fasttext'
EMBEDDING_DIM = 100
MID_TERM_NFILTERS=10#EMBEDDING_DIM
LONG_TERM_NFILTERS=4
embedding_size_ntn=100
tensor_dim_ntn=EMBEDDING_DIM
TRAIN_TEST_SEP_RATE = .8
dropout_ntn=0.3

# train base config
BATCH_SIZE = 256
NUM_EPOCH = 400

## optimizer
LEARNING_RATE= 0.0005
BETA1 = 0.9
BETA2 = 0.999
EPSILON = 1e-08
DECAY = 0.01
LEARNING_RATE_NTN=0.0005
    
## constrain
REGULARIZER_WEIGHT = 0.001
REGULARIZER_WEIGHT_NTN=0.0001
# lms_cnn_base
OUTPUT_DIM = 2

DENSE_HIDDEN_SIZE = 16
NEIGHBORHOOD_COMBINE = 3

SHORT_TERM_LENGTH = 1
MID_TERM_LENGTH = 7
LONG_TERM_LENGTH = 30

## lms_cnn
MID_TERM_CONV_KERNEL = (NEIGHBORHOOD_COMBINE, EMBEDDING_DIM)
MID_TERM_POOL_SIZE = (3,2)#every 3 filters / 2 days
LONG_TERM_CONV_KERNEL = (NEIGHBORHOOD_COMBINE, EMBEDDING_DIM)
LONG_TERM_POOL_SIZE = (2,7)#every 2 filters /7 days

DENSE_HIDDEN_INPUT = int(EMBEDDING_DIM) + int((MID_TERM_LENGTH - NEIGHBORHOOD_COMBINE + 1)/MID_TERM_POOL_SIZE[1])*int(MID_TERM_NFILTERS/MID_TERM_POOL_SIZE[0]) + int((LONG_TERM_LENGTH - NEIGHBORHOOD_COMBINE + 1)/LONG_TERM_POOL_SIZE[1])*int(LONG_TERM_NFILTERS/LONG_TERM_POOL_SIZE[0]) 

# print DENSE_HIDDEN_INPUT

lms_cnn_base_config = [ EMBEDDING_METHOD, EMBEDDING_DIM, BATCH_SIZE,
	NEIGHBORHOOD_COMBINE, LEARNING_RATE,REGULARIZER_WEIGHT]


lsm_cnn_config = lms_cnn_base_config +[DENSE_HIDDEN_SIZE, MID_TERM_CONV_KERNEL,MID_TERM_POOL_SIZE,
																						LONG_TERM_CONV_KERNEL, LONG_TERM_POOL_SIZE]

# this keeps config that must unchanged for the same model to be load
lsm_cnn_config_save_model = [ EMBEDDING_DIM,
														NEIGHBORHOOD_COMBINE,
														DENSE_HIDDEN_SIZE, MID_TERM_CONV_KERNEL,MID_TERM_POOL_SIZE,
																						LONG_TERM_CONV_KERNEL, LONG_TERM_POOL_SIZE]